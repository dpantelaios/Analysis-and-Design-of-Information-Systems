[agent]
  interval = "1s"
  flush_interval = "1s"

# [[inputs.cpu]]
#   percpu = false
#   totalcpu = true

[[outputs.http]]
  url = "http://grafana-server2:3000/api/live/push/custom_stream_id"
  data_format = "influx"
  [outputs.http.headers]
    Authorization = "Bearer eyJrIjoiWGlhSEhYQTFuUVhrSlNyY2FickNCZGxmUHljbE9ibWIiLCJuIjoiYXBpLWtleSIsImlkIjoxfQ=="

# [[outputs.websocket]]
#   url = "wss://grafana-server2:3000/api/live/push/custom_stream_id"
#   data_format = "influx"
#   [outputs.websocket.headers]
#     Authorization = "Bearer eyJrIjoieXhXU3RBM1RHY0dBY0ZEMzFHYjlGcUc5c3JwNW9TUkEiLCJuIjoiYXBpLWtleSIsImlkIjoxfQ=="

[[outputs.influxdb]]
  urls = ["http://influxdb:8086"]
  database = "influx"
  timeout = "15s"
  username = "telegraf"
  password = "metricsmetricsmetricsmetrics"
  data_format = "influx"


[[outputs.file]]
  ## Files to write to, "stdout" is a specially handled file.
  files = ["stdout", "/tmp/metrics.out"]

  ## Data format to output.
  ## Each data format has its own unique set of configuration options, read
  ## more about them here:
  ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_OUTPUT.md
  data_format = "json"

  ## Maximum line length in bytes.  Useful only for debugging.
  influx_max_line_bytes = 0

  ## When true, fields will be output in ascending lexical order.  Enabling
  ## this option will result in decreased performance and is only recommended
  ## when you need predictable ordering while debugging.
  influx_sort_fields = false

  ## When true, Telegraf will output unsigned integers as unsigned values,
  ## i.e.: `42u`.  You will need a version of InfluxDB supporting unsigned
  ## integer values.  Enabling this option will result in field type errors if
  ## existing data has been written.
  influx_uint_support = false

[[inputs.kafka_consumer]]
  # name_override = "th1_RAW"
  ## Kafka brokers.
  brokers = ["kafka:29090"]
  topics = ['RAW']
  # consumer_group = "test-1"
  max_message_len = 10000000
  # data_format = "influx"
  data_format = "json"
  # data_type = "string"
  # tags = ["value"]
  # name_prefix = "bla"
  # name_suffix = "bla"
  json_time_key = "produceDate"
  json_time_format = "unix_ms"
  # json_string_fields  = ['key']
  tag_keys = ['sensorName']
  # tagpass = {sensor = ["th1"]}
  json_name_key = "sensorName"


[[inputs.kafka_consumer]]
  # name_override = "AGGREGATED"
  ## Kafka brokers.
  brokers = ["kafka:29090"]
  topics = ['AGGREGATED_SUM']
  max_message_len = 1000000
  # data_format = "influx"
  data_format = "json"
  json_time_key = "sumDate"
  json_time_format = "unix_ms"
  tag_keys = ['sensorName']
  json_name_key = "sensorName"
  name_prefix = "AGGREGATED_SUM_"

[[inputs.kafka_consumer]]
  ## Kafka brokers.
  brokers = ["kafka:29090"]
  topics = ['AGGREGATED_AVERAGE']
  max_message_len = 1000000
  data_format = "json"
  json_time_key = "aggregationDate"
  json_time_format = "unix_ms"
  tag_keys = ['sensorName']
  json_name_key = "sensorName"
  name_prefix = "AGGREGATED_AVERAGE_"
  
[[inputs.kafka_consumer]]
  ## Kafka brokers.
  brokers = ["kafka:29090"]
  topics = ['DAILY_RAW']
  max_message_len = 1000000
  data_format = "json"
  json_time_key = "produceDate"
  json_time_format = "unix_ms"
  tag_keys = ['sensorName']
  json_name_key = "sensorName"

[[inputs.kafka_consumer]]
  name_override = "AGGREGATED_DIFF"
  ## Kafka brokers.
  brokers = ["kafka:29090"]
  # topics = ['RAW']
  topics = ['AGGREGATED_DIFF']
  max_message_len = 1000000
  # data_format = "json"
  data_format = "json"
    # data_type = "string"


